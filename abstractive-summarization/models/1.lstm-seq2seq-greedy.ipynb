{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "4395 4395\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ctexts_cn_path = '/home/wjunneng/Ubuntu/2019-NLP/abstractive-summarization/dataset/ctexts_cn.txt'\n",
    "ctexts_en_path = '/home/wjunneng/Ubuntu/2019-NLP/abstractive-summarization/dataset/ctexts_en.json'\n",
    "headlines_cn_path = '/home/wjunneng/Ubuntu/2019-NLP/abstractive-summarization/dataset/headlines_cn.txt'\n",
    "headlines_en_path = '/home/wjunneng/Ubuntu/2019-NLP/abstractive-summarization/dataset/headlines_en.json'\n",
    "\n",
    "def get_topic_modelling(ctexts_path, headlines_path, n=500):\n",
    "    \"\"\"\n",
    "    获取topic的字词\n",
    "    :param text_line: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ctexts = []\n",
    "    headlines = []\n",
    "    with open(file=ctexts_path, encoding='utf-8-sig', mode='r') as file:\n",
    "        ctexts_filelines = json.load(file)\n",
    "    with open(file=headlines_path, encoding='utf-8-sig', mode='r') as file:\n",
    "        headlines_filelines = json.load(file)\n",
    "\n",
    "    for index in range(len(ctexts_filelines)):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            # sparse matrix 矩阵\n",
    "            tf_matrix = vectorizer.fit_transform(raw_documents=[ctexts_filelines[index]])\n",
    "            # 字词\n",
    "            tf_features = vectorizer.get_feature_names()\n",
    "            # 提取特征\n",
    "            tf_svd = TruncatedSVD(1).fit(X=tf_matrix)\n",
    "            # add\n",
    "            ctexts.append(\" \".join([tf_features[i] for i in tf_svd.components_[0].argsort()[: -n-1: -1]]))\n",
    "            # headline\n",
    "            headlines.append(headlines_filelines[index])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return ctexts, headlines\n",
    "            \n",
    "ctexts, headlines = get_topic_modelling(ctexts_en_path, headlines_en_path)\n",
    "print(len(ctexts), len(headlines))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "unk_count: 0\nvocab from size: 49585\nMost common words [('dot', 4394), ('the', 4379), ('comma', 4349), ('to', 4280), ('in', 4268), ('of', 4262)]\nSample data [5, 7, 4, 6, 10, 9, 11, 8, 15, 2062] ['the', 'to', 'dot', 'comma', 'and', 'of', 'on', 'in', 'was', 'festival']\nunk_count: 0\nvocab to size: 8534\nMost common words [('to', 1388), ('in', 1196), ('comma', 876), ('s', 785), ('for', 733), ('of', 596)]\nSample data [2797, 14, 2798, 2799, 656, 2800, 5, 1642, 657, 2086] ['daman', 'and', 'diu', 'revokes', 'mandatory', 'rakshabandhan', 'in', 'offices', 'order', 'malaika']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def build_vocabulary(words:list, n_words:int):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    count.extend(Counter(words).most_common(n_words))\n",
    "    \n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 1)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    print('unk_count:', unk_count)\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    rev_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, rev_dictionary\n",
    "\n",
    "concat_from = ' '.join(ctexts).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_vocabulary(concat_from, vocabulary_size_from)\n",
    "\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
    "\n",
    "concat_to = ' '.join(headlines).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_vocabulary(concat_to, vocabulary_size_to)\n",
    "\n",
    "print('vocab to size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in range(len(headlines)):\n",
    "    headlines[i] = headlines[i] + ' EOS'\n",
    "\n",
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']\n",
    "\n",
    "def str_idx(corpus, dictionary, UNK=3):\n",
    "    X = []\n",
    "    for words in corpus:\n",
    "        x_idx = []\n",
    "        for word in words.split():\n",
    "            x_idx.append(dictionary.get(word, UNK))\n",
    "        X.append(x_idx)\n",
    "    \n",
    "    return X\n",
    "\n",
    "X, Y = str_idx(ctexts, dictionary_from), str_idx(headlines, dictionary_to)\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8b8836cd",
   "language": "python",
   "display_name": "PyCharm (NLP-Models-Tensorflow)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}