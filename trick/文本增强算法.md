# 									**文本增强算法**

## 一、应用场景

### 		1、少样本场景

	简介：	
		
		该场景下能够收集到的数据不足以满足模型训练的需求。
		
	常用模型/算法：

### 		2、分类任务中类别不均衡的场景

	简介：	
	
		该场景下模型对小样本类别往往处于欠拟合的状态，导致实际预测时，几乎不会给予该类别太高的概率。因此，通常对小样本类别进行数据增强，降低样本间的不均衡性，提高模型的泛化性能。
		
	常用模型/算法：

### 		3、半监督训练场景


	简介：
	
		该场景下通过在无标签样本上作用数据增强，以构造半监督训练所需的样本对，让模型从无标签的数据中获取到优化所需的梯度。
		
	常用模型/算法：
		UDA

### 4、提高模型的鲁棒性场景

```
简介（不严谨的情况）：

		该场景下一类是，保持语义不变，变换文本的表达形式，如回译、文本复述等；另一类是，按照某种策略对文本进行局部调整，如同义词替换、随机删除等。这两种都可以认为是提高了模型的鲁棒性，使得模型更加关注文本的语义信息，并对文本的局部噪声不再敏感。

常用模型/算法：
	TTA
```

## 二、典型技术方案

### 1、回译（Back Translation）

```
简介：
	
	利用翻译模型将语种1的原始文本翻译为语种2的文本表达，再将语种2的表达翻译为语种3的文本表达，最终再将语种3的形式翻译为语种1的文本表达，此文本即为经过文本增强后的文本。因此，翻译模型的好坏决定了数据增强的最终效果。
	
	第一，如何采用翻译模型，可以使用random sample或者beam search等策略实现成倍数的数据扩充。如果使用google等翻译工具，可以更改中间语种，也可以实现N倍的扩充。
	
	第二，现在的翻译模型对长文本的翻译能力较弱，因此，通常是按照“。”符号对长文本切割成一条条的短句子，然后进行回译操作，拼接成新的文本。
	
	第三，回译技术的有效性本质上来源于迁移学习。通过文本增强的过程，回译技术将翻译模型学习到的关于词义，语法，句法等知识迁移到新的文本上，从而为当前的自然语言处理任务引入了新的信息和知识来源。
	
	第四，回译技术产生的新样本如果有效，蕴含着这样一个先验，即模型对于具有不用语言表达形式但是具备同样的语义信息的输入文本，应该具备不变性，或者具有相近的输出。那么是否所有的自然语言处理任务，都具备这样一个先验？
```

### 2、随机词替换（Randomly Replace/EDA）

```
简介：
	
	通常是随机地选择文本中一定比例的词，并对这些词进行同义词替换，随机删除等操作。不同于回译技术的是，并不需要外部训练好的模型的辅助。
	
	EDA（Easy data augmentation）包括了同义词替换，随机交换，随机插入，随机删除。详细说明如下：
	
	·同义词替换（Synonyms Replace）：从句子中随机选择非停用词，然后用随机的同义词进行替换。
	·随机插入（Randomly Insert）：随机地从句中找出不属于停用词集的词，并求出其随机的同一词，并将该同义词随机地插入到句中的一个随机位置，重复N次。
	·随机交换（Randomly Swap）：随机地选择句中的两个单词，交换他们的位置，重复N次。
	·随机删除（Randomly Delete）：以概率p随机删除句中的每一个单词。
	
	·SR缺点：同义词具备非常相似的词向量，而训练模型时这两个句子会被当作几乎相同的句子，但在实际上并没有对数据进行有效的扩充。
	·RI缺点：原本的训练数据丧失了语义结构和语义顺序，同时，不考虑停用词的做法使得生成的增强文本并没有包含太多有价值的信息，同义词的加入并没有侧重句子中的关键词，在数据扩充的多样性上实际受限较多。
	·RS缺点：实际上并没有改变原句的词素，对新句式，句型，相似词的泛化能力上提升很有限。
	·RD缺点：关键词没有侧重，句式句型泛化效果差。随机的方法固然能够兼顾到所有的词，但是没有关键词的侧重，因此，如果删除的词刚好是任务中特征最强的词，那么不仅语义信息可能被改变，任务目标的准确性也会出现问题。
```

### 3、非核心词替换（基于TF-IDF的词替换）

```
简介：
	
	EDA技术中，对于要替换的词是随机选取的，因此直观的感受是，如果是一些重要的词被替换的话，那么增强后的文本的质量将大打折扣，因此提出“基于非核心词替换的数据增强技术”。即，用词典中不重要的词去替换文本中一定比例的不重要的词，从而产生新的文本。

常用模型/算法：	
	TF-IDF
```

### 4、基于上下文信息的文本增强（C-BERT）

```
简介：
	
	首先，训练好一个语言模型（LM），对于需要增强的文本，随机替换掉文中的一个词或字（这取决于LM支持的是字还是词），接下来，将文本中的剩余部分输入到语言模型，选择语言模型所预测的top k个词去替换原文中被去掉的词，形成k条新的文本。
	
常用模型/算法：	
	CBERT
```

### 5、基于语言生成模型的文本增强（LAMBADA）

```
简介：
	
	首先，在大量文本上进行预训练，使得模型能够捕捉语言的结构，从而能产生连贯的句子。然后在不同任务的少量数据集上进行微调，并使用微调后的模型生成新的句子。最后，在相同的小型数据集上训练分类器，并进行过滤，保证现有的小型数据集和新生成的数据集有相近的分布。

常用模型/算法：
	LAMBADA
```

## 三、深度学习数据增强技术

### 1、Mixmatch

```
简介：
	
	采用Mixup方法猜测数据增强产生的无标签样本的低熵标签，并且把无标签数据与有标签数据混合起来。可以在差分隐私的使用目的下，在准确率和隐私保护之间取得较好的平衡。

```

